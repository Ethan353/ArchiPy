---
description:
globs:
alwaysApply: true
---
Rule: Core principles for AI-assisted code generation and refactoring based on Martin Fowler's Refactoring

## General Code Generation Guidelines
1. **Small, Behavior-Preserving Steps**: Generate code in small, incremental changes that preserve existing functionality. Ensure each change can be validated via end-to-end tests.
2. **Clear Naming**: Use descriptive, intention-revealing names for variables, functions, and classes. Follow Pydantic naming conventions (e.g., `CAMEL_CASE` for fields in `BaseConfig`).
3. **No Duplication**: Eliminate duplicated code by extracting shared logic into reusable functions, methods, or Pydantic models. Check for existing utilities in `base_utils.py` before generating new code.
4. **Single Responsibility**: Ensure functions, classes, and Pydantic models have a single, clear purpose. Split large components (e.g., long functions or complex models) into smaller, focused units.
5. **Minimize Mutable Data**: Prefer immutable data structures in Pydantic models (e.g., use `frozen=True` in `BaseDTO`). Encapsulate mutable data in functions or objects to limit scope.
6. **Avoid Over-Engineering**: Do not add speculative generality or unnecessary abstractions unless explicitly required by the current feature or configuration.

## Refactoring-Specific Rules
7. **BDD-Driven Refactoring**: Before generating or modifying code, ensure corresponding Gherkin feature files exist in the Behave framework to validate behavior. Generate or update `.feature` files to describe end-to-end behavior.
8. **Frequent End-to-End Testing**: After each code change, prompt to run relevant Behave scenarios. Highlight test failures and suggest reverting to the last working state if tests fail.
9. **Address Code Smells**:
   - **Mysterious Name**: Rename variables, functions, or Pydantic fields to clearly communicate intent (e.g., `HOSTS` in `ElasticSearchConfig` should reflect its purpose).
   - **Long Function**: Break functions longer than 15 lines into smaller, single-purpose functions, especially in utility classes like `BaseUtils`.
   - **Long Parameter List**: Group related parameters into Pydantic models (e.g., as done in `PostgresSQLAlchemyConfig`).
   - **Global Data**: Encapsulate global variables, such as `BaseConfig.__global_config`, to restrict access (e.g., via getter methods).
   - **Data Clumps**: Combine co-occurring fields into Pydantic models (e.g., `USERNAME`, `PASSWORD`, `HOST` in `SQLAlchemyConfig`).
   - **Feature Envy**: Move functions to the module they interact with most (e.g., move utility methods in `BaseUtils` to relevant configuration classes if they heavily depend on them).
   - **Comments**: Refactor code to be self-explanatory instead of relying on comments, especially in Pydantic models where field descriptions can suffice.
10. **Opportunistic Refactoring**: When generating new code, check for opportunities to improve existing code (e.g., simplify `BaseConfig` or `BaseUtils`) without adding new functionality.
11. **Leave Code Healthier**: Ensure every generated change improves readability, maintainability, or structure compared to the existing codebase (e.g., align with `BaseDTO`’s `frozen=True`).

## Pydantic and Pydantic Settings Integration
12. **Pydantic Model Consistency**: Generate Pydantic models inheriting from `BaseDTO`
13. **Settings Source Priority**: Respect the configuration source priority defined in `BaseConfig` (pyproject.toml, configs.toml, .env, OS env vars, class defaults) when generating configuration-related code.
14. **Validation Logic**: Include `@model_validator` methods for complex validation (e.g., as in `ElasticSearchConfig` or `KafkaConfig`) to ensure configuration integrity.
15. **Secret Handling**: Use `SecretStr` for sensitive fields (e.g., `HTTP_PASSWORD`, `SECRET_KEY`) and avoid exposing them in logs or string representations.
16. **Type Safety**: Use strict typing with `typing` module (e.g., `Generic`, `Literal`, `TypeVar`) as seen in `base_config.py` and `base_dtos.py` to ensure type safety.

## BDD and Testing with Behave
17. **Gherkin Scenarios**: Generate Gherkin `.feature` files for new functionality, following the structure of existing Behave tests. Ensure scenarios cover happy paths, edge cases, and failure conditions.
18. **End-to-End Focus**: Prioritize generating end-to-end tests over unit tests, aligning with Behave’s BDD approach. Map Gherkin steps to step definitions in `steps/` directory.
19. **Test Failure Feedback**: If a Behave scenario fails, suggest specific fixes or revert to the last known good state. Include the failing scenario and step in explanations.
20. **Test Coverage**: Ensure generated Gherkin scenarios test critical areas (e.g., configuration loading in `BaseConfig`, validation in `BaseUtils`) and boundary conditions.

## Code Style and Structure
21. **Consistent Formatting**: Follow PEP 8 and project-specific style (e.g., use `pydantic` conventions like `Field` with `description`). Use tools like Black or Ruff for formatting.
22. **Modular Design**: Organize code into modules based on functionality (e.g., keep configuration in `archipy.configs`, utilities in `archipy.helpers.utils`). Avoid generating code that causes divergent changes.
23. **Error Handling**: Include comprehensive error handling in generated code, using custom exceptions from `archipy.models.errors` (e.g., `InvalidPhoneNumberError`).
24. **Documentation**: Use Pydantic field `description` attributes for documentation instead of inline comments. Generate docstrings for complex methods, following the style in `base_utils.py`.

## Performance and Optimization
25. **Defer Performance Optimization**: Do not optimize for performance unless profiling data indicates a bottleneck. Focus on clarity and maintainability in Pydantic models and utilities.
26. **Measure Before Optimizing**: If performance optimization is requested, suggest using a profiler (e.g., `cProfile`) to identify bottlenecks and target only those areas.

## Workflow Integration
27. **Version Control**: After generating or refactoring code, prompt to commit changes with a Conventional Commits message (e.g., `feat: add new configuration`, `refactor: simplify BaseUtils`).
28. **Context Awareness**: Use context from `base_config.py`, `config_template.py`, `base_dtos.py`, and `base_utils.py` to ensure generated code aligns with existing patterns (e.g., Pydantic model structure, utility methods).
29. **Review Changes**: Present generated code as a diff (red for deletions, green for additions) and require manual review before applying. Avoid automatically applying large changes.

## AI Behavior
30. **Proactive Suggestions**: Identify and suggest fixes for code smells in the surrounding codebase (e.g., simplify `BaseUtils` methods, reduce complexity in `BaseConfig`).
31. **Explain Decisions**: Provide explanations for generated code, referencing specific rules or project conventions (e.g., “Used `SecretStr` for `PASSWORD` per Rule 15”).
32. **Avoid Hallucinations**: Only generate code based on provided files (`base_config.py`, `config_template.py`, `base_dtos.py`, `base_utils.py`) or explicitly referenced dependencies. Do not invent APIs or configurations.
33. **Handle Large Refactorings**: For multi-file changes, break tasks into smaller steps, generate changes file by file, and verify consistency with `BaseConfig` and `BaseDTO`.

## Example Prompt Response Template
When responding to a code generation or refactoring prompt:
1. Analyze provided files (`base_config.py`, `config_template.py`, `base_dtos.py`, `base_utils.py`) for patterns and conventions.
2. Generate code adhering to Pydantic, Pydantic Settings, and Behave conventions, following the above rules.
3. Present changes as a diff with explanations referencing specific rules (e.g., “Refactored to eliminate duplication per Rule 3”).
4. Suggest generating or updating Gherkin `.feature` files and running Behave tests to validate changes.
5. Prompt to commit changes with a descriptive message.
6. If uncertain, ask for clarification or additional context (e.g., specific configuration requirements or Behave scenarios).
